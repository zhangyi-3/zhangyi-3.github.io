<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Zhang Yi</title>

    <meta name="author" content="Zhang Yi">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <!--    <link rel="icon" type="image/png" href="images/seal_icon.png">-->
    
    <!-- Mobile detection script -->
    <script type="text/javascript">
        // Check if device is mobile
        window.isMobile = function() {
            return /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent) || window.innerWidth < 768;
        };
        
        // When page loads, check if mobile and adjust accordingly
        window.addEventListener('DOMContentLoaded', function() {
            if (isMobile()) {
                // Adjust comparison sliders for mobile
                var sliders = document.getElementsByTagName('img-comparison-slider');
                for (var i = 0; i < sliders.length; i++) {
                    sliders[i].setAttribute('hover', 'false');
                }
                
                // Fix project images size
                var oneElements = document.getElementsByClassName('one');
                var twoElements = document.getElementsByClassName('two');
                
                for (var i = 0; i < oneElements.length; i++) {
                    var width = oneElements[i].offsetWidth;
                    oneElements[i].style.height = 'auto';
                }
                
                for (var i = 0; i < twoElements.length; i++) {
                    var width = twoElements[i].offsetWidth;
                    twoElements[i].style.height = 'auto';
                }
            }
        });
    </script>
</head>


<script defer src="https://unpkg.com/img-comparison-slider@7/dist/index.js"></script>
<link rel="stylesheet" href="https://unpkg.com/img-comparison-slider@7/dist/styles.css"/>

<style>
    .slider-example-split-line {
        --divider-width: 4px;
        --divider-color: #ffa658;
        --default-handle-opacity: 0;
        max-width: 100%;
        height: auto;
        border-radius: 8px;
        overflow: hidden;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    
    @media only screen and (max-width: 768px) {
        img-comparison-slider {
            max-width: 100% !important;
            width: 100% !important;
            height: auto !important;
        }
        
        img-comparison-slider img {
            max-width: 100% !important;
            width: 100% !important;
            height: auto !important;
        }
    }
</style>

<style>
    .before,
    .after {
        margin: 0;
    }

    .before figcaption,
    .after figcaption {
        background: #fff;
        border: 1px solid #c0c0c0;
        border-radius: 12px;
        color: #2e3452;
        opacity: 0.8;
        padding: 12px;
        position: absolute;
        top: 50%;
        transform: translateY(-50%);
        line-height: 100%;
        font-weight: 500;
        box-shadow: 0 2px 5px rgba(0,0,0,0.05);
        transition: all 0.3s ease;
    }
    
    img-comparison-slider:hover .before figcaption,
    img-comparison-slider:hover .after figcaption {
        opacity: 1;
    }

    .before figcaption {
        left: 12px;
    }

    .after figcaption {
        right: 12px;
    }
    
    @media only screen and (max-width: 768px) {
        .before figcaption,
        .after figcaption {
            padding: 8px;
            font-size: 12px;
        }
    }
</style>

<body>
<table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr style="padding:0px;background: linear-gradient(to right, rgba(240,240,245,0.5), rgba(255,255,255,0));">
                    <td style="padding:2.5%;width:63%;vertical-align:middle">
                        <p style="text-align:center">
                            <name>Zhang Yi</name>
                        </p>
                        <p style="line-height:1.5;margin-top:15px;margin-bottom:15px;">
                            I am currently a Senior Researcher at <a href="https://vivix.ai/">Vivix AI</a>.
                            I obtained my Ph.D. in 2023 from the <a href="https://mmlab.ie.cuhk.edu.hk/">MMLab</a> at The Chinese University of Hong Kong (CUHK),
                            where I was fortunate to be advised by <a href="https://www.ee.cuhk.edu.hk/~hsli/">Prof. Hongsheng Li</a> and
                            <a href="https://scholar.google.com/citations?user=-B5JgjsAAAAJ">Prof. Xiaogang Wang</a>.
                            Prior to that, I earned my bachelor's degree from <a href="https://www.nju.edu.cn/EN/main.htm">Nanjing University</a> in 2019.

                        </p>
                        <p style="text-align:center;margin-top:25px;">
                            <a href="mailto:zhangyi3.link@gmail.com" style="margin:0 5px;display:inline-block;padding:5px 10px;border-radius:4px;background-color:rgba(0,0,0,0.02);transition:all 0.3s ease;">Email</a> &nbsp/&nbsp
                            <a href="https://scholar.google.com/citations?user=zQmprrUAAAAJ" style="margin:0 5px;display:inline-block;padding:5px 10px;border-radius:4px;background-color:rgba(0,0,0,0.02);transition:all 0.3s ease;">Google Scholar</a>
                            &nbsp/&nbsp
                            <a href="https://github.com/zhangyi-3/" style="margin:0 5px;display:inline-block;padding:5px 10px;border-radius:4px;background-color:rgba(0,0,0,0.02);transition:all 0.3s ease;">Github</a>&nbsp/&nbsp
                            <a href="https://drive.google.com/file/d/1Ru91vvckCTydahj-sDqf1zwzZY4V347d/view?usp=sharing" style="margin:0 5px;display:inline-block;padding:5px 10px;border-radius:4px;background-color:rgba(0,0,0,0.02);transition:all 0.3s ease;">CV</a>
                        </p>
                    </td>
                    <td style="padding:2.5%;width:30%;max-width:30%">
                        <a href="images/zhangyi3_circle.png"><img style="width:100%;max-width:100%;border-radius:50%;box-shadow:0 4px 10px rgba(0,0,0,0.1);" alt="profile photo"
                                                           src="images/zhangyi3_circle.png" class="hoverZoomLink"></a>
                    </td>
                </tr>
                </tbody>
            </table>
            <div class="hiring-callout">
                <strong>Hiring interns:</strong> diffusion distillation and long-video generation optimization. Email <a href="mailto:zhangyi3.link@gmail.com">zhangyi3.link@gmail.com</a>.
            </div>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>Research</heading>
                        <p>
                            My current research interests focus on multimodal models, diffusion models, and computational photography.
                            I am passionate about conducting research with real-world impact, addressing pressing technological challenges.
                            Most of my publications have been successfully integrated into commercial products.
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>

                    <tr onmouseout="neiGRPO_stop()" onmouseover="neiGRPO_start()">
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <div class="one">
                                <div class="two" id="neiGRPO_image" style="opacity: 0;">
                                    <img style="width:100%" src="images/neiGRPO_after.jpg"></div>
                                <img style="width:100%" src="images/neiGRPO_before.jpg">
                            </div>
                            <script type="text/javascript">
                                function neiGRPO_start() {
                                    document.getElementById('neiGRPO_image').style.opacity = "1";
                                }
    
                                function neiGRPO_stop() {
                                    document.getElementById('neiGRPO_image').style.opacity = "0";
                                }
    
                                neiGRPO_stop()
                            </script>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <a href="https://dl.acm.org/doi/abs/10.1145/3687995">
                                <papertitle>Neighbor GRPO: Contrastive ODE Policy Optimization Aligns Flow Models</papertitle>
                            </a>
                            <br>
                            <a href="https://scholar.google.com/citations?user=f5MTTy4AAAAJ">Dailan He</a>,
                            <a href="">Guanlin Feng</a>,
                            <a href="https://xingtongge.github.io/">Xingtong Ge</a>,
                            <a href="https://scholar.google.com/citations?user=P3BUrBQAAAAJ&hl=zh-TW">Yazhe Niu</a>,
                            <strong>Yi Zhang</strong>,
                            <a href="https://mabingqi.github.io/">Bingqi Ma</a>,
                            <a href="https://scholar.google.com/citations?user=Bd3v08QAAAAJ&hl=zh-CN&oi=ao">Guanglu Song</a>,
                            <a href="https://liuyu.us/">Yu Liu</a>,
                            <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>
                            <br>
                            <em>Arxiv</em>, 2026
                            <br>
                            <a href="https://arxiv.org/pdf/2511.16955">arXiv</a> 
                            <p></p>
                            <p>Neighbor GRPO enables GRPO for flow matching models by bypassing SDEs entirely. 
                                By reinterpreting SDE-based GRPO from a distance optimization perspective, we generate diverse trajectories via perturbed initial noise 
                                and optimize using a distance-based surrogate policy, preserving ODE efficiency while outperforming SDE-based methods.</p>
                        </td>
                    </tr>

                    <tr onmouseout="senseflow_stop()" onmouseover="senseflow_start()">
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <div class="one">
                                <div class="two" id="senseflow_image" style="opacity: 0;">
                                    <img style="width:100%" src="images/senseflow_after.jpg"></div>
                                <img style="width:100%" src="images/senseflow_before.jpg">
                            </div>
                            <script type="text/javascript">
                                function senseflow_start() {
                                    document.getElementById('senseflow_image').style.opacity = "1";
                                }
    
                                function senseflow_stop() {
                                    document.getElementById('senseflow_image').style.opacity = "0";
                                }
    
                                senseflow_stop()
                            </script>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <a href="https://dl.acm.org/doi/abs/10.1145/3687995">
                                <papertitle>SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image Distillation</papertitle>
                            </a>
                            <br>
                            <a href="https://xingtongge.github.io/">Xingtong Ge</a>,
                            <a href="">Xin Zhang</a>,
                            <a href="https://tongdaxu.github.io/">Tongda Xu</a>,
                            <strong>Yi Zhang</strong>,
                            <a href="https://xinjie-q.github.io/"> Xinjie Zhang</a>,
                            <a href="https://yanwang202199.github.io/">Yan Wang</a>,
                            <a href="https://eejzhang.people.ust.hk/">Jun Zhang</a>
                            <br>
                            <em>ICLR</em>, 2026
                            <br>
                            <a href="https://arxiv.org/pdf/2506.00523">arXiv</a> /
                            <a href="https://huggingface.co/domiso/SenseFlow">huggingface</a> /
                            <a href="https://github.com/XingtongGe/SenseFlow">code</a>
                            <p></p>
                            <p>We address convergence difficulties of Distribution Matching Distillation (DMD) on large-scale flow-based models. 
                                By proposing implicit distribution alignment (IDA) and intra-segment guidance (ISG), SenseFlow enables DMD to converge for SD 3.5 and FLUX.1 dev, 
                                achieving superior distillation performance for both diffusion and flow-matching models.</p>
                        </td>
                    </tr>

                    <tr onmouseout="ccm_stop()" onmouseover="ccm_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="ccm_image" style="opacity: 0;">
                                <img style="width:100%" src="images/ccm_after.jpg"></div>
                            <img style="width:100%" src="images/ccm_before.jpg">
                        </div>
                        <script type="text/javascript">
                            function ccm_start() {
                                document.getElementById('ccm_image').style.opacity = "1";
                            }

                            function ccm_stop() {
                                document.getElementById('ccm_image').style.opacity = "0";
                            }

                            ccm_stop()
                        </script>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://dl.acm.org/doi/abs/10.1145/3687995">
                            <papertitle>See Further When Clear: Curriculum Consistency Model</papertitle>
                        </a>
                        <br>
                        <a href="">Yunpeng Liu</a>,
                        <a href="https://scholar.google.com/citations?user=-zEM0ycAAAAJ&hl=zh-CN&oi=ao">Boxiao Liu</a>,
                        <strong>Yi Zhang</strong>,
                        <a href="">Xingzhong Hou</a>,
                        <a href="https://scholar.google.com/citations?user=Bd3v08QAAAAJ&hl=zh-CN&oi=ao">Guanglu Song</a>,
                        <a href="https://liuyu.us/">Yu Liu</a>,
                        <a href="">Haihang You</a>
                        <br>
                        <em>CVPR</em>, 2025
                        <br>
                        <a href="https://arxiv.org/pdf/2412.06295">arXiv</a> /
                        <a href="https://openreview.net/attachment?id=xVOMtecrAS&name=supplementary_material">supplement</a>
                        <!-- <a href="https://perspective-armirror.github.io/">code</a> -->
                        <p></p>
                        <p>The study introduces the Curriculum Consistency Model (CCM) to address inconsistent learning complexity in 
                            Consistency Distillation for diffusion and flow models, balancing complexity via a PSNR-based curriculum.</p>
                    </td>
                </tr>

                <tr onmouseout="armirror_stop()" onmouseover="armirror_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="armirror_image" style="opacity: 0;">
                                <img style="width:100%" src="images/armirror_after.jpg"></div>
                            <img style="width:100%" src="images/armirror_before.jpg">
                        </div>
                        <script type="text/javascript">
                            function armirror_start() {
                                document.getElementById('armirror_image').style.opacity = "1";
                            }

                            function armirror_stop() {
                                document.getElementById('armirror_image').style.opacity = "0";
                            }

                            armirror_stop()
                        </script>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://dl.acm.org/doi/abs/10.1145/3687995">
                            <papertitle>Perspective-Aligned AR Mirror with Under-Display Camera</papertitle>
                        </a>
                        <br>
                        <a href="https://jianwang-cmu.github.io/">Jian Wang</a>,
                        <a href="https://sizhuoma.netlify.app/">Sizhuo Ma</a>,
                        <a href="">Karl Bayer</a>,
                        <strong>Yi Zhang</strong>,
                        <a href="https://scholar.google.com/citations?user=fqf2tBsAAAAJ&hl=en">Peihao Wang</a>,
                        <a href="https://scholar.google.com/citations?user=xc4mjhkAAAAJ&hl=en&authuser=1">Bing Zhou</a>,
                        <a href="https://scholar.google.com/citations?user=3pZs3j0AAAAJ&hl=en">Shree Nayar</a>,
                        <a href="">Gurunandan Krishnan</a>
                        <br>
                        <em>SIGGRAPH Asia</em>, 2024,  &nbsp; (Journal)  &nbsp; <font color="red"><strong>(Best Paper Award)</strong></font>
                        <br>
                        <a href="https://dl.acm.org/doi/abs/10.1145/3687995">arXiv</a> /
                        <a href="https://jianwang-cmu.github.io/24ARmirror/supp.pdf">supplement</a> /
                        <a href="https://perspective-armirror.github.io/">code</a>
                        <p></p>
                        <p>A novel AR mirror system that allows a seamless, perspective-aligned user experience, which is enabled by
                            placing the camera behind a transparent display.</p>
                    </td>
                </tr>

                <tr onmouseout="motioni2v_stop()" onmouseover="motioni2v_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="motioni2v_image" style="opacity: 0;">
                                <img style="width:100%" src="images/motioni2v.gif"></div>
                            <img style="width:100%" src="images/motioni2v.gif">
                        </div>
                        <script type="text/javascript">
                            function motioni2v_start() {
                                document.getElementById('motioni2v_image').style.opacity = "1";
                            }

                            function motioni2v_stop() {
                                document.getElementById('motioni2v_image').style.opacity = "0";
                            }

                            motioni2v_stop()
                        </script>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2401.15977">
                            <papertitle>Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling</papertitle>
                        </a>
                        <br>
                        <a href="https://scholar.google.com/citations?user=fbEuTJUAAAAJ">Xiaoyu Shi</a>,
                        <a href="https://drinkingcoder.github.io/">Zhaoyang Huang</a>,
                        <a href="https://g-u-n.github.io/">Fu-Yun Wang</a>,
                        <a href="https://wkbian.github.io/">Weikang Bian</a>,
                        <a href="https://scholar.google.com/citations?user=ZJLzmwgAAAAJ">Dasong Li</a>,
                        <strong>Yi Zhang</strong>,
                        <a href="https://manyuan97.github.io/">Manyuan Zhang</a>,
                        <a href="">Ka Chun Cheung</a>,
                        <a href="">Simon See</a>,
                        <a href="https://scholar.google.com/citations?user=ZGM7HfgAAAAJ">Honwei Qin</a>,
                        <a href="https://scholar.google.com.hk/citations?user=SH_-B_AAAAAJ&hl=zh-CN">Jifeng Dai</a>,
                        <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>
                        <br>
                        <em>SIGGRAPH</em>, 2024
                        <br>
                        <a href="https://arxiv.org/abs/2401.15977">arXiv</a> /
                        <a href="https://github.com/G-U-N/Motion-I2V">code</a>
                        <p></p>
                        <p>We introduce Motion-I2V, a novel framework for consistent and controllable image-to-video generation (I2V).
                            In contrast to previous methods that directly learn the complicated image-to-video mapping, Motion-I2V
                            factorizes I2V into two stages with explicit motion modeling.</p>
                    </td>
                </tr>

                <tr onmouseout="UCDIR_stop()" onmouseover="UCDIR_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle">

                        <img-comparison-slider tabindex="0" class="slider-example-split-line rendered" hover="hover">
                            <figure slot="first" class="before">
                                <img src="images/UCDIR_before.jpg" width=100% height=100%>
                            </figure>
                            <figure slot="second" class="after">
                                <img src="images/UCDIR_after.jpg" width=100% height=100%>
                            </figure>
                        </img-comparison-slider>

                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2303.02881">
                            <papertitle>A Unified Conditional Framework for Diffusion-based Image Restoration
                            </papertitle>
                        </a>
                        <br>
                        <strong>Yi Zhang</strong>,
                        <a href="https://scholar.google.com/citations?user=fbEuTJUAAAAJ">Xiaoyu Shi</a>,
                        <a href="https://scholar.google.com/citations?user=ZJLzmwgAAAAJ">Dasong Li</a>,
                        <a href="https://scholar.google.com/citations?user=-B5JgjsAAAAJ">Xiaogang Wang</a>,
                        <a href="https://jianwang-cmu.github.io/">Jian Wang</a>,
                        <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>
                        <br>
                        <em>NeurIPS</em>, 2023
                        <br>

                        <a href="https://arxiv.org/abs/2305.20049">arXiv</a> /
                        <a href="https://github.com/zhangyi-3/UCDIR">code</a> /
                        <a href="https://zhangyi-3.github.io/project/UCDIR/">project</a>
                        <p></p>
                        <p>A unified conditional framework based on diffusion models for image restoration.</p>
                    </td>
                </tr>


                <tr onmouseout="kbnet_stop()" onmouseover="kbnet_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="kbnet_image" style="opacity: 0;">
                                <img style="width:100%" src="images/kbnet_after.jpg"></div>
                            <img style="width:100%" src="images/kbnet_before.jpg">
                        </div>
                        <script type="text/javascript">
                            function kbnet_start() {
                                document.getElementById('kbnet_image').style.opacity = "1";
                            }

                            function kbnet_stop() {
                                document.getElementById('kbnet_image').style.opacity = "0";
                            }

                            kbnet_stop()
                        </script>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2303.02881">
                            <papertitle>KBNet: Kernel Basis Network for Image Restoration</papertitle>
                        </a>
                        <br>
                        <strong>Yi Zhang</strong>,
                        <a href="https://scholar.google.com/citations?user=ZJLzmwgAAAAJ">Dasong Li</a>,
                        <a href="https://scholar.google.com/citations?user=fbEuTJUAAAAJ">Xiaoyu Shi</a>,
                        <a href="https://scholar.google.com/citations?user=f5MTTy4AAAAJ">Dailan He</a>,
                        <a href="">Kangning Song</a>,
                        <a href="https://scholar.google.com/citations?user=-B5JgjsAAAAJ">Xiaogang Wang</a>,
                        <a href="https://scholar.google.com/citations?user=ZGM7HfgAAAAJ">Honwei Qin</a>,
                        <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>
                        <br>
                        <em>Arxiv</em>, 2023
                        <br>
                        <a href="https://arxiv.org/abs/2303.02881">arXiv</a> /
                        <a href="https://github.com/zhangyi-3/kbnet">code</a>
                        <p></p>
                        <p>A general-purpose backbone for image restoration tasks (e.g. denoising, deraining, and
                            deblurring).
                            A novel kernel basis attention (KBA) module has been proposed to effectively aggregate the
                            spatial information via a
                            series of learnable kernel bases.</p>
                    </td>
                </tr>

                <tr onmouseout="group_shift_stop()" onmouseover="group_shift_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="group_shift_image" style="opacity: 0;">
                                <img style="width:100%" src="images/group_shift_after.jpg"></div>
                            <img style="width:100%" src="images/group_shift_before.jpg">
                        </div>
                        <script type="text/javascript">
                            function group_shift_start() {
                                document.getElementById('group_shift_image').style.opacity = "1";
                            }

                            function group_shift_stop() {
                                document.getElementById('group_shift_image').style.opacity = "0";
                            }

                            group_shift_stop()
                        </script>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="">
                            <papertitle>No Attention is Needed: Grouped Spatial-temporal Shift for Simple and Efficient
                                Video Restorers
                            </papertitle>
                        </a>
                        <br>
                        <a href="https://scholar.google.com/citations?user=ZJLzmwgAAAAJ">Dasong Li</a>,
                        <a href="https://scholar.google.com/citations?user=fbEuTJUAAAAJ">Xiaoyu Shi</a>,
                        <strong>Yi Zhang</strong>,
                        <a href="https://scholar.google.com/citations?user=-B5JgjsAAAAJ">Xiaogang Wang</a>,
                        <a href="https://scholar.google.com/citations?user=ZGM7HfgAAAAJ">Honwei Qin</a>,
                        <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>
                        <br>
                        <em>CVPR</em>, 2023
                        <br>
                        <a href="https://arxiv.org/abs/2206.10810">arXiv</a> /
                        <a href="https://github.com/dasongli1/grouped_shift_net">code</a> /
                        <a href="https://dasongli1.github.io/publication/grouped-shift-net/">project</a>
                        <p></p>
                        <p>We propose a grouped spatial-temporal shift module for effective video restoration.
                            It surpasses previous SOTA methods with only 43% computational cost on video deblurring and
                            denoising.
                        </p>
                    </td>
                </tr>


                <tr onmouseout="burst_raw_vst_stop()" onmouseover="burst_raw_vst_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="burst_raw_vst_image" style="opacity: 0;">
                                <img style="width:100%" src="images/burst_raw_vst_after.jpg"></div>
                            <img style="width:100%" src="images/burst_raw_vst_before.jpg">
                        </div>
                        <script type="text/javascript">
                            function burst_raw_vst_start() {
                                document.getElementById('burst_raw_vst_image').style.opacity = "1";
                            }

                            function burst_raw_vst_stop() {
                                document.getElementById('burst_raw_vst_image').style.opacity = "0";
                            }

                            burst_raw_vst_stop()
                        </script>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="">
                            <papertitle>Efficient Burst Raw Denoising with Variance Stabilization and Multi-frequency
                                Denoising Network
                            </papertitle>
                        </a>
                        <br>
                        <a href="https://scholar.google.com/citations?user=ZJLzmwgAAAAJ">Dasong Li*</a>,
                        <strong>Yi Zhang*</strong>,
                        <a href="">Ka Lung Law</a>,
                        <a href="https://scholar.google.com/citations?user=-B5JgjsAAAAJ">Xiaogang Wang</a>,
                        <a href="https://scholar.google.com/citations?user=ZGM7HfgAAAAJ">Honwei Qin</a>,
                        <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>
                        <br>
                        <em>IJCV, 2022</em>
                        <br>
                        <a href="https://arxiv.org/abs/2205.04721">arXiv</a>
                        <p></p>
                        <p>A practical low-light imaging system for burst raw image denoising.
                            It has been deployed to some commercial smartphones.</p>
                    </td>
                </tr>

                <tr onmouseout="blur_representation_stop()" onmouseover="blur_representation_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="blur_representation_image" style="opacity: 0;">
                                <img style="width:100%" src="images/blur_representation_after.jpg"></div>
                            <img style="width:100%" src="images/blur_representation_before.jpg">
                        </div>
                        <script type="text/javascript">
                            function blur_representation_start() {
                                document.getElementById('blur_representation_image').style.opacity = "1";
                            }

                            function blur_representation_stop() {
                                document.getElementById('blur_representation_image').style.opacity = "0";
                            }

                            blur_representation_stop()
                        </script>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="">
                            <papertitle>Learning Degradation Representations for Image Deblurring</papertitle>
                        </a>
                        <br>
                        <a href="https://scholar.google.com/citations?user=ZJLzmwgAAAAJ">Dasong Li</a>,
                        <strong>Yi Zhang</strong>,
                        <a href="">Ka Chun Cheung</a>,
                        <a href="https://scholar.google.com/citations?user=-B5JgjsAAAAJ">Xiaogang Wang</a>,
                        <a href="https://scholar.google.com/citations?user=ZGM7HfgAAAAJ">Honwei Qin</a>,
                        <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>
                        <br>
                        <em>ECCV</em>, 2022
                        <br>
                        <a href="https://arxiv.org/abs/2208.05244">arXiv</a> /
                        <a href="https://github.com/dasongli1/Learning_degradation/">code</a>
                        <p></p>
                        <p>Learning degradation representations for blurry images. The learned degradation
                            representation
                            shows pleasing improvements on both image deblurring and reblurring.

                        </p>
                    </td>
                </tr>

                <tr onmouseout="IDR_stop()" onmouseover="IDR_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="IDR_image" style="opacity: 0;">
                                <img style="width:100%" src="images/IDR_after.jpg"></div>
                            <img style="width:100%" src="images/IDR_before.jpg">
                        </div>
                        <script type="text/javascript">
                            function IDR_start() {
                                document.getElementById('IDR_image').style.opacity = "1";
                            }

                            function IDR_stop() {
                                document.getElementById('IDR_image').style.opacity = "0";
                            }

                            IDR_stop()
                        </script>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="">
                            <papertitle>Self-Supervised Image Denoising via Iterative Data Refinement</papertitle>
                        </a>
                        <br>
                        <strong>Yi Zhang</strong>,
                        <a href="https://scholar.google.com/citations?user=ZJLzmwgAAAAJ">Dasong Li</a>,
                        <a href="">Ka Lung Law</a>,
                        <a href="https://scholar.google.com/citations?user=-B5JgjsAAAAJ">Xiaogang Wang</a>,
                        <a href="https://scholar.google.com/citations?user=ZGM7HfgAAAAJ">Honwei Qin</a>,
                        <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>
                        <br>
                        <em>CVPR</em>, 2022
                        <br>
                        <a href="https://arxiv.org/abs/2111.14358">arXiv</a> /
                        <a href="https://github.com/zhangyi-3/IDR">code</a> /
                        <a href="https://github.com/zhangyi-3/IDR">SenseNoise dataset</a>
                        <p></p>
                        <p>A self-supervised image denoising method using noisy images and the noise model.
                            We achieve SOTA results on both real-world noise and synthetic noises (both point-wise and
                            spatial correlated noise types).
                        </p>
                    </td>
                </tr>

                <tr onmouseout="noise_synthesis_stop()" onmouseover="noise_synthesis_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="noise_synthesis_image" style="opacity: 0;">
                                <img style="width:100%" src="images/noise_synthesis_after.jpg"></div>
                            <img style="width:100%" src="images/noise_synthesis_before.jpg">
                        </div>
                        <script type="text/javascript">
                            function noise_synthesis_start() {
                                document.getElementById('noise_synthesis_image').style.opacity = "1";
                            }

                            function noise_synthesis_stop() {
                                document.getElementById('noise_synthesis_image').style.opacity = "0";
                            }

                            noise_synthesis_stop()
                        </script>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="">
                            <papertitle>Rethinking Noise Synthesis and Modeling in Raw Denoising</papertitle>
                        </a>
                        <br>
                        <strong>Yi Zhang</strong>,
                        <a href="https://scholar.google.com/citations?user=ZGM7HfgAAAAJ">Honwei Qin</a>,
                        <a href="https://scholar.google.com/citations?user=-B5JgjsAAAAJ">Xiaogang Wang</a>,
                        <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>
                        <br>
                        <em>ICCV</em>, 2021
                        <br>
                        <a href="https://arxiv.org/abs/2110.04756">arXiv</a> /
                        <a href="https://github.com/zhangyi-3/noise-synthesis">code</a>
                        <p></p>
                        <p>A method to synthesize "almost the most realistic" raw image noise by sampling directly from
                            the real noise distribution.
                            We also found existing comparisons of noise synthesis methods are based on inaccurate noise
                            parameters.</p>
                    </td>
                </tr>


                <tr onmouseout="starcraft_stop()" onmouseover="starcraft_start()">
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="starcraft_image" style="opacity: 0;">
                                <img style="width:100%" src="images/starcraft_after.jpg"></div>
                            <img style="width:100%" src="images/starcraft_before.jpg">
                        </div>
                        <script type="text/javascript">
                            function starcraft_start() {
                                document.getElementById('starcraft_image').style.opacity = "1";
                            }

                            function starcraft_stop() {
                                document.getElementById('starcraft_image').style.opacity = "0";
                            }

                            starcraft_stop()
                        </script>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/1809.09095">
                            <papertitle>On Reinforcement Learning for Full-length Game of StarCraft</papertitle>
                        </a>
                        <br>
                        <a href="">Zhen-Jia Pang</a>,
                        <a href="">Ruo-Ze Liu</a>,
                        <a href="">Zhou-Yu Meng</a>,
                        <strong>Yi Zhang</strong>,
                        <a href="http://www.lamda.nju.edu.cn/yuy/">Yang Yu</a>,
                        <a href="">Tong Lu</a>
                        <br>
                        <em>AAAI</em>, 2019 &nbsp <font color="red"><strong>(Oral)</strong></font>
                        <br>
                        <a href="https://arxiv.org/abs/1809.09095">arXiv</a> /
                        <a href="https://www.bilibili.com/video/av22575605?zw">demo</a>
                        <p></p>
                        <p>We achieve over 93% winning rate of Protoss against the most difficult non-cheating
                            built-in AI (level-7) of Terran,
                            training within two days using a single machine with only 48 CPU cores and 8 K40 GPUs.</p>
                    </td>
                </tr>


                </tbody>
            </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>

                <td width="100%" valign="middle">
                    <heading>Academic Services</heading>
                    <p>
                    </p>
                    <li>Conference Reviewer: CVPR, ICCV, ECCV, ICML, NeurIPS, ICLR, AAAI</li>
                    <li>Journal Reviewer: TIP</li>
                    <p></p>
                </td>
                </tbody>
            </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>

                <td width="100%" valign="middle">
                    <heading>Experience</heading>
                    <p>
                    </p>
                    <li>Senior Researcher at Vivix AI, 2025.1 ~ present</li>
                    <li>Senior Researcher at SenseTime, 2023.10 ~ 2024.12</li>
                    <li>Research intern at Snap Research, 2023.3 ~ 2023.6</li>
                    <li>Research intern at LAMDA Group, 2017 ~ 2018</li>
                    <p></p>
                </td>
                </tbody>
            </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>


                <td width="100%" valign="middle">
                    <heading>Honors</heading>
                    <p>
                    </p>
                    <li>Best Paper Award, SIGGRAPH Asia, 2024</li>
                    <li>Postgraduate Scholarship, the Chinese University of Hong Kong, 2019 ~ 2023</li>
                    <li>Outstanding Graduate of Nanjing University, 2019</li>
                    <li>Merit student in Jiangsu Province, 2018</li>
                    <li>National Scholarship, 2018</li>
                    <p></p>
                </td>
                </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:0px">
                        <br>
                        <p style="text-align:right;font-size:small;">
                            The website template was borrowed from <a
                                href="https://github.com/jonbarron/jonbarron_website">here</a>.
                            <br>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>
        </td>
    </tr>
</table>
</body>

</html>